{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Tutorial\n",
    "\n",
    "OpenCV was started at Intel in 1999 by Gary Bradsky, and the first release came out in 2000. \n",
    "\n",
    "- It is a Python library which is designed to solve computer vision problems. OpenCV was originally developed in 1999 by Intel but later it was supported by Willow Garage.\n",
    "\n",
    "- It supports a wide variety of programming languages such as C++, Python, Java etc. Support for multiple platforms including Windows, Linux, and MacOS.\n",
    "\n",
    "- OpenCV Python is nothing but a wrapper class for the original C++ library to be used with Python. Using this, all of the OpenCV array structures gets converted to/from NumPy arrays. This makes it easier to integrate it with other libraries which use NumPy. For example, libraries such as SciPy and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 1:** How to read, write, and display Images & Videos\n",
    "We will set up a basic understanding of how we can begin learning image processing with OpenCV by reading, writing, and displaying images as well videos using OpenCV methods like imshow(), imread(), etc. \n",
    "\n",
    "### **Chapter 2:** Basic Functions\n",
    "After we have established a basic understanding, we will learn about various methods on images to resize, blur, etc.\n",
    "\n",
    "### **Chapter 3:** Shapes and Texts\n",
    "We will learn about the various shapes that we can put the images into and with corresponding text values as their labels\n",
    "\n",
    "### **Chapter 4:** Joining Images\n",
    "This section particularly focuses on how we can join the images together.\n",
    "### **Chapter 5:** Mini-Projects - \n",
    "### - Face Detection on Images, Videos, & in Real-time\n",
    "### - Face and Eyes Detection in Real-time\n",
    "### - Pedestrians Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OpenCV Github Link** https://github.com/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.0\n"
     ]
    }
   ],
   "source": [
    "import cv2 #Import the OpenCV Library\n",
    "import numpy as np\n",
    "\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 1\n",
    "\n",
    "### How to read, write, and display Images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying an Image\n",
    "\n",
    "img = cv2.imread('dog.jpg')\n",
    "#imread enables us to read an Image\n",
    "#The 2nd argumnet is a flag which specifies the way image should be read\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "cv2.waitKey(0) #Delay. No. of Milliseconds for which we want to show our image window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('copy_dog.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read, write, and display Videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying a Video\n",
    "\n",
    "cap = cv2.VideoCapture(0) #If you want to display the Video File, just give the name of it inplace of 0\n",
    "\n",
    "# is a 4-byte identifier which specifies the format of a video stream\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('X','V','I','D') \n",
    "\n",
    "out = cv2.VideoWriter('Name of Output File.avi', fourcc, 20.0, (640,480)) #No. of frames/s, Frame Size\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to capture the frame continuously\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read() #frame variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "    \n",
    "    if success == True:\n",
    "        \n",
    "        out.write(frame)\n",
    "        \n",
    "        cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "        if cv2.waitKey(1) == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading & Displaying a Video\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(3, 640)#Width\n",
    "cap.set(4, 480)#Height\n",
    "cap.set(10, 100)#brightness\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to go through each frame\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read() #img variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "    cv2.imshow(\"Video\", img)\n",
    "      \n",
    "   \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "        break\n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 2\n",
    "\n",
    "### Basic Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the Image into Gray Scale\n",
    "\n",
    "#Reading an Image\n",
    "\n",
    "img = cv2.imread('dog.jpg') \n",
    "\n",
    "imgGray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)#Converting the Image into Gray Scale\n",
    "\n",
    "cv2.imshow('GrayScale Image',imgGray)\n",
    "\n",
    "cv2.imwrite('copy_lena.png', imgGray)\n",
    "\n",
    "\n",
    "\n",
    "#imgGray1 = cv2.cvtColor(imgGray, cv2.COLOR_GRAY2RGB)#Converting the Image into Gray Scale\n",
    "\n",
    "#cv2.imshow('GrayScale Image1',imgGray1)\n",
    "\n",
    "\n",
    "cv2.waitKey(0) #Delay. No. of Milliseconds for which we want to show our image window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blur Function to Blur the Image\n",
    "\n",
    "#Reading an Image\n",
    "\n",
    "img = cv2.imread('dog.jpg') \n",
    "\n",
    "imgBlur = cv2.GaussianBlur(img, (21,21), 0)\n",
    "#(7,7) is the Kernel Size (Amount of Blur) which is always odd. So, (3,3), (5,5), etc.\n",
    "# 0 is Sig function (Standard Deviation along X-Direction) \n",
    "\n",
    "cv2.imshow('Blurred Image',imgBlur)\n",
    "\n",
    "cv2.waitKey(0) #Delay. No. of Milliseconds for which we want to show our image window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge Detector - Canny \n",
    "\n",
    "img = cv2.imread('dog.jpg') #Reading an Image\n",
    "\n",
    "imgCanny = cv2.Canny(img, 150, 200)#Threshold Values\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "#Type of the object which is unsigned integer of 8 bits which means the values can range from 0 to 255\n",
    "\n",
    "imgDilation = cv2.dilate(imgCanny, kernel, iterations = 1)#dilate function is used when the Edges are not properly connected\n",
    "\n",
    "imgErosion = cv2.erode(imgDilation, kernel, iterations = 1)#Erosion function is used when we want to thin (Erode) the Image\n",
    "\n",
    "\n",
    "#iterations - How much thickness do we actually need?\n",
    "\n",
    "cv2.imshow('Canny Image',imgCanny)\n",
    "cv2.imshow('Dilation Image',imgDilation)\n",
    "cv2.imshow('Erosion Image',imgErosion)\n",
    "\n",
    "\n",
    "cv2.waitKey(0) #Delay. 0 Means Infinite Delay. No. of Milliseconds for which we want to show our image window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "#Resizing the Image\n",
    "\n",
    "#Reading & Displaying an Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('dog.jpg') \n",
    "#Reading an Image\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "print(img.shape) #Shape of the Image (Height, Width, No.of Channels(RGB))\n",
    "\n",
    "cv2.waitKey(0) #Delay. 0 Means Infinite Delay. No. of Milliseconds for which we want to show our image window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = cv2.resize(img, (200, 400)) # (Width, Height)\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Re-sized Image\", img_resize) #Displaying the Image\n",
    "\n",
    "cv2.waitKey(0) #Delay. 0 Means Infinite Delay. If specified 1, it means 1 milliseconds\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "#Cropping the Image\n",
    "\n",
    "img = cv2.imread('dog.jpg') #Reading an Image\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "imgCropped = img[0:200, 200:500] #(Height, Width)\n",
    "\n",
    "cv2.imshow(\"Cropped Image\", imgCropped) #Displaying the Image\n",
    "\n",
    "print(img.shape) #Shape of the Image (Height, Width, No.of Channels(BGR))\n",
    "\n",
    "cv2.waitKey(0) #Delay. 0 Means Infinite Delay. If specified 1, it means 1 milliseconds\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 3\n",
    "\n",
    "### Shapes and Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "#0 means Black\n",
    "\n",
    "img = np.zeros((512, 512))#Print a Black Image. It is a GrayScale Image\n",
    "\n",
    "cv2.imshow(\"O\", img)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 means Black\n",
    "\n",
    "img = np.zeros((512, 512, 3),np.uint8 )#Print a Black Image with 3 Channles RGB.\n",
    "\n",
    "#img[:] = 255, 0 , 0\n",
    "\n",
    "cv2.line(img, (0,0), (300,300), (0,255,255),3) #Staring Pt., Ending Pt., Color, Thickness\n",
    "\n",
    "cv2.imshow(\"O\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(img, (0,0), (250,350), (0, 255, 255), cv2.FILLED) #cv2.FILLED is used to Fill the Rectangles\n",
    "\n",
    "cv2.imshow(\"O\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.circle(img, (400,50), 30 , (0, 255, 255), cv2.FILLED) #Center, Radius , Color\n",
    "\n",
    "cv2.imshow(\"O\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.putText(img, \"OpenCV\", (300, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 150,0), 1) #Staring Pt., Font, Scale, Color, Thickness\n",
    "\n",
    "cv2.imshow(\"O\", img)\n",
    "\n",
    "#print(img.shape)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 4\n",
    "\n",
    "### Joining Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to Join Image Together\n",
    "\n",
    "img = cv2.imread('dog.jpg') #Reading an Image\n",
    "\n",
    "imgHor = np.hstack((img, img))#Images stacked in Horizonatl Direction\n",
    "\n",
    "cv2.imshow(\"Horizontal\", imgHor) #Displaying the Image\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to Join Image Together\n",
    "\n",
    "img = cv2.imread('dog.jpg') #Reading an Image\n",
    "\n",
    "imgVer = np.vstack((img, img))#Images stacked in Horizonatl Direction\n",
    "\n",
    "cv2.imshow(\"Vertical\", imgVer) #Displaying the Image\n",
    "\n",
    "\n",
    "#Both the Images must have same Channels because they have a matrix and we cannot resize the Images\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 5\n",
    "\n",
    "### Face Detection on an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "img = cv2.imread('face.jpg') #Reading an Image\n",
    "\n",
    "img = cv2.resize(img, (1240,640))\n",
    "imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x,y),(x+w,y+h),(0,0,255),2)\n",
    "\n",
    "cv2.imshow(\"Output\", img) #Displaying the Image\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection in Videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('vid.mp4')\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to capture the frame continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read() #frame variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "            \n",
    "        \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "        break\n",
    "            \n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection in Real-Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0) #If you want to display the Video File, just give the name of it inplace of 0\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to capture the frame continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read() #frame variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "            \n",
    "        \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),3)\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "        break\n",
    "            \n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face & Eyes Detection in Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face and Eyes Detection in Real-Time\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to capture the frame continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_eye.xml\")\n",
    "faceCascade1 = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read() #frame variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "            \n",
    "        \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    eyes = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "    faces = faceCascade1.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,0),2)\n",
    "    \n",
    "    for (x, y, w, h) in eyes:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,0),2)\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "        break\n",
    "            \n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestrians Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('vtest.avi') #note - insert a video link here\n",
    "\n",
    "#Videos are just a sequence of Images\n",
    "#So, we will add a while loop to capture the frame continuously\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_fullbody.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read() #frame variable will capture the Video & success variable will tell us whether it was captured successfully or not\n",
    "            \n",
    "        \n",
    "    imgGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(imgGray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,0),2)\n",
    "    \n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'): #This adds a Delay and looks for the key press inorder to break the loop\n",
    "        break\n",
    "            \n",
    "        \n",
    "cap.release() #Release the resources after recording\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
